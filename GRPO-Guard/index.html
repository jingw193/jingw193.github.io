<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GRPO-Guard: Mitigating Implicit Reward Hacking</title>
    <link rel="icon" type="image/png" href="assets/logo.png">
    
    <style>
        /* --- General Styles & Typography --- */
        body {
            font-family: 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            margin: 0;
            background-color: #fdfdfd;
        }
        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
        }
        h1, h2, h3 {
            color: #1a1a1a;
            font-weight: 600;
        }
        h1 { font-size: 2.4em; }
        h2 { font-size: 1.8em; border-bottom: 2px solid #eee; padding-bottom: 10px; margin-top: 40px; }
        h3 { font-size: 1.4em; border: none; }
        a { color: #007bff; text-decoration: none; }
        a:hover { text-decoration: underline; }
        p { margin-bottom: 1em; }
        img { max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); }
        .caption {
            font-size: 0.9em;
            color: #666;
            text-align: center;
            margin-top: 10px;
            margin-bottom: 30px;
        }

        /* --- ================================== --- */
        /* --- == NEW: Hero Header Styles == --- */
        /* --- ================================== --- */
        .hero-header {
            background-color: #f8f9fa; /* A professional, light gray */
            padding: 40px 20px;
            text-align: center;
            border-bottom: 1px solid #e7e7e7;
        }
        .hero-header .logo {
            height: 100px; /* Greatly increased logo size */
            width: auto;
            box-shadow: none;
            margin-bottom: 25px; /* Space between logo and title */
        }
        .hero-header h1 {
            border: none;
            padding: 0;
            margin-top: 0;
        }
        .hero-header .authors { font-size: 1.1em; margin: 15px 0; line-height: 1.8; }
        .hero-header .affiliations { color: #555; margin-bottom: 15px; font-size: 0.95em; }
        .hero-header .footnotes { font-size: 0.9em; color: #555; }
        .hero-header .links { margin-top: 25px; }
        .hero-header .links a {
            margin: 0 15px;
            font-weight: bold;
            font-size: 1.2em;
            padding: 8px 15px;
            border: 1px solid #007bff;
            border-radius: 5px;
            transition: all 0.2s ease-in-out;
        }
        .hero-header .links a:hover {
            background-color: #007bff;
            color: white;
            text-decoration: none;
        }

        /* --- Other Sections --- */
        .abstract {
            background-color: #f9f9f9;
            border-left: 5px solid #007bff;
            padding: 20px;
            margin: 40px 0;
            border-radius: 5px;
        }
        .abstract p { font-size: 1.1em; }
        .grid { display: grid; gap: 20px; margin-top: 20px; }
        .grid-1-col { grid-template-columns: 1fr; }
        .grid-item { text-align: center; }
        .bibtex {
            background-color: #f0f0f0;
            padding: 15px;
            border-radius: 5px;
            white-space: pre-wrap;
            word-wrap: break-word;
            font-family: 'Courier New', Courier, monospace;
        }
    </style>
</head>
<body>

    <header class="hero-header">
        <img src="assets/logo.png" alt="GRPO Guard Logo" class="logo">
        <h1>GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping</h1>
        
        <div class="authors">
            <p>
                Jing Wang<sup>1,2</sup>,
                Jiajun Liang<sup>2,*</sup>,
                Jie Liu<sup>3</sup>,
                Henglin Liu<sup>2,4</sup>,
                Gongye Liu<sup>2,5</sup>,
                Jun Zheng<sup>1</sup>,
                Wanyuan Pang<sup>6</sup>, <br>
                Ao Ma<sup>7</sup>,
                Zhenyu Xie<sup>1</sup>,
                Xintao Wang<sup>2</sup>,
                Meng Wang<sup>2</sup>,
                Pengfei Wan<sup>2</sup>,
                Xiaodan Liang<sup>1,â€ </sup>
            </p>
        </div>
        <div class="affiliations">
            <p>
                <sup>1</sup>Shenzhen Campus of Sun Yat-Sen University &nbsp;&nbsp;
                <sup>2</sup>Kling Team, Kuaishou Technology &nbsp;&nbsp;
                <sup>3</sup>CUHK MMLab <br>
                <sup>4</sup>Tsinghua University &nbsp;&nbsp;
                <sup>5</sup>HKUST &nbsp;&nbsp;
                <sup>6</sup>USTB &nbsp;&nbsp;
                <sup>7</sup>UCAS
            </p>
        </div>
        <div class="footnotes">
             <p><sup>*</sup>Project Leader. &nbsp;&nbsp; <sup>â€ </sup>Corresponding Author.</p>
        </div>
        <div class="links">
            <a href="#">[ðŸ“„ Paper]()</a>
            <a href="https://github.com/yifan123/flow_grpo">[ðŸ’» Code]</a>
        </div>
    </header>

    <div class="container">
        
        <section id="teaser">
            <img src="assets/figure1.png" alt="Comparison between FlowGRPO and GRPO-Guard">
            <p class="caption">
                <b>Figure 1:</b> While standard FlowGRPO (top) suffers from severe reward hackingâ€”leading to degraded diversity, quality, and text-image consistencyâ€”our GRPO-Guard (bottom) maintains high visual quality and a stable reward score.
            </p>
        </section>

        <section id="overview">
            <h2>Overview</h2>
            <p>GRPO-Guard mitigates implicit over-optimization in FlowGRPO, preventing degradation of image quality. Watch our video presentation for a quick summary of our key contributions.</p>
            <div class="video-container">
                <video controls width="100%" style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                    <source src="assets/video1.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </div>
        </section>

        <section id="abstract">
            <div class="abstract">
                <h2>Abstract</h2>
                <p>
                    Recently, GRPO-based reinforcement learning has shown remarkable progress in optimizing flow-matching models, effectively improving their alignment with task-specific rewards.
                    Within these frameworks, the policy update relies on importance-ratio clipping to constrain overconfident positive and negative gradients.
                    However, in practice, we observe a systematic shift in the importance-ratio distributionâ€”its mean falls below 1 and its variance differs substantially across timesteps. This left-shifted and inconsistent distribution prevents positive-advantage samples from entering the clipped region, causing the mechanism to fail in constraining overconfident positive updates.
                    As a result, the policy model inevitably enters an <strong>implicit over-optimization</strong> stageâ€”while the proxy reward continues to increase, essential metrics such as image quality and textâ€“prompt alignment deteriorate sharply, ultimately making the learned policy impractical for real-world use.
                    To address this issue, we introduce <strong>GRPO-Guard</strong>, a simple yet effective enhancement to existing GRPO frameworks. Our method incorporates ratio normalization, which restores a balanced and step-consistent importance ratio, ensuring that PPO clipping properly constrains harmful updates across denoising timesteps. In addition, a gradient reweighting strategy equalizes policy gradients over noise conditions, preventing excessive updates from particular timestep regions. 
                    Together, these designs act as a regulated clipping mechanism, stabilizing optimization and substantially mitigating implicit over-optimization without relying on heavy KL regularization.
                    Extensive experiments on multiple diffusion backbones (e.g., SD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard significantly reduces over-optimization while maintaining or even improving generation quality. These results highlight GRPO-Guard as a robust and general solution for stable policy optimization in flow-matching models.
                </p>
            </div>
        </section>

        <section id="problem">
            <h2>The Problem: When Reward Goes Up, Quality Goes Down</h2>
            <p>
                In standard GRPO frameworks, the model quickly enters an over-optimization phase. The proxy reward (e.g., text accuracy) keeps increasing, but the true "gold" reward (human-perceived quality) plummets. This results in visually corrupted images, loss of detail, and strange artifacts, making the models unusable in practice. Below are examples of this failure mode.
            </p>
            <div class="grid grid-1-col">
                <div class="grid-item">
                    <img src="assets/failure_example_1.png" alt="FlowGRPO Failure Example 1">
                </div>
                <div class="grid-item">
                    <img src="assets/failure_example_2.png" alt="DanceGRPO Failure Example 2">
                </div>
            </div>
            <p class="caption">
                <b>Visual comparison on the GenEval and OCR tasks.</b> While FlowGRPO and DanceGRPO show severe quality degradation and fail to follow instructions, our GRPO-Guard (third and fifth rows) maintains high fidelity and instruction-following capabilities.
            </p>
        </section>

        <section id="solution">
            <h2>Our Solution: Restoring Balance to Training</h2>
            <p>
                GRPO-Guard tackles the root cause of reward hacking with two key components. First, <strong>Ratio Normalization</strong> corrects the distribution of importance-sampling ratios, ensuring the clipping mechanism functions as intended. Second, <strong>Gradient Reweighting</strong> balances updates across all timesteps, preventing the model from focusing too heavily on a single noise level.
            </p>
            <img src="assets/figure2_graphs.png" alt="Comparison of importance ratio distributions">
            <p class="caption">
                <b>Up (FlowGRPO):</b> The importance ratio distribution is shifted and inconsistent, disabling the clipping mechanism. <b>Down (GRPO-Guard):</b> Our method restores a balanced and stable distribution, enabling proper optimization.
            </p>
        </section>

        <section id="results">
            <h2>Visual Comparisons</h2>
            <h3>Preventing Over-optimization During Training</h3>
            <p>The following shows how image quality evolves during training. FlowGRPO quickly degenerates, while our method produces consistently high-quality results.</p>
            <img src="assets/figure8_timeline.png" alt="Training timeline comparison">
            <p class="caption">Comparison at different training steps. Top row is FlowGRPO, bottom row is ours.</p>

            <h3>Preserving Human Likeness</h3>
            <p>When optimizing for aesthetic scores, baseline methods often distort human proportions and reduce facial diversity. GRPO-Guard avoids this, preserving realistic body structures.</p>
            <img src="assets/figure7.png" alt="Preserving human likeness">
            <p class="caption">Comparison of human likeness preservation. Top row is FlowGRPO, bottom row is ours.</p>
        </section>

        <section id="human-eval">
            <h2>Human Evaluation</h2>
            <p>We conducted a human preference evaluation, and the results clearly demonstrate the superiority of GRPO-Guard in image quality, text alignment, and overall preference.</p>
            <img src="assets/figure10_human_eval.png" alt="Human evaluation results" style="width: 60%; margin: 0 auto; display: block;">
             <p class="caption">Win/tie/lose ratios comparing GRPO-Guard against baseline methods. Our method is strongly preferred by human evaluators.</p>
        </section>

        <section id="citation">
            <h2>Citation</h2>
            <p>If you find our work useful, please consider citing:</p>
            <div class="bibtex">
@article{wang2024flowgrpoguard,
  title={GRPO-Guard: Mitigating Implicit Reward Hacking in Flow-GRPO via Balanced Clipping},
  author={Wang, Jing and Liang, Jiajun and Liu, Jie and Liu, Henglin and Liu, Gongye and Zheng, Jun and Pang, Wanyuan and Ma, Ao and Xie, Zhenyu and Wang, Xintao and others},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2024}
}
            </div>
        </section>

    </div>
</body>
</html>